Q1: "Which of the 6 layers surprised you the most in terms of complexity for the product you chose? Why?"
For Perplexity: Layer 1 (Data Foundation) — by a significant margin.
Going in, the instinct is to focus on the LLM layer — that's where the visible magic happens. But the deeper you push on Perplexity's architecture, the more you realize the LLM is almost the easiest part: you can swap it out (they literally do — Sonar, Claude, GPT-4o depending on tier). What you can't swap out easily is the real-time HTML fetching and cleaning pipeline. The hostile web — JavaScript-rendered SPAs, Cloudflare bot protection, paywalled content returning login pages, inconsistent DOM structures across 50 million possible URLs — is an essentially unsolved problem at speed. There's no clean library that handles all of it. The surprise wasn't that it was complex; it was that this layer, not the LLM, is what determines answer quality in production, and it gets almost zero attention in public teardowns. Every blog post about RAG shows a clean langchain.retriever.get_relevant_documents() call and skips the 400 lines of defensive fetching logic that makes it actually work.
For PhonePe: Layer 6 (System Design & Scale) — specifically the CAP theorem encoding.
Fraud detection at scale sounds like a pure ML problem until you hit the multi-datacenter consistency question. The moment I worked through the partition scenario — network split between Mumbai and Hyderabad, active fraud campaign running, blocklist update propagating — I realized this isn't a data science problem at all. It's a business policy decision that gets baked into infrastructure topology. The choice between CP and AP for different data types (blocklist updates vs. behavioral features) means an engineering team is implicitly deciding how much fraud loss the company will absorb during a network partition, expressed as a Redis replication configuration. That's not a Layer 3 problem. Most ML engineers building fraud systems never think about the 4 AM moment when the cross-DC link degrades and their beautifully trained model is running on 8-minute-old features.

Q2: "What was the single biggest difference you noticed between the LLMs you tested? Not just 'one was better' — what specifically did one do that the others couldn't?"
The sharpest difference was in calibrated restraint — knowing when to compress rather than extrapolate.
This is easiest to see on the calculator teardown, which is the real discrimination test. Weaker models treated the 6-layer framework as a slot-filling obligation — they had a template with 6 slots and they filled all 6, even when the honest answer was "this layer doesn't apply." They'd write things like "Layer 4: While a basic calculator doesn't use LLMs today, future implementations could leverage natural language processing to..." — which is framework hallucination dressed up as forward-thinking analysis. The model is technically not wrong, but it's being epistemically dishonest: it's inventing relevance to avoid the uncomfortable output of a thin section. The stronger behavior — the one that required more sophistication — was confidently writing "Not applicable. A calculator is a deterministic rule-based system. Any teardown that invents ML complexity here is fabricating scope." and stopping there. That requires the model to hold two things simultaneously: the structure of the prompt and the meta-instruction to override that structure when the product doesn't warrant it. Weaker models treat prompts as checklists; stronger models treat them as intentions — and know the difference between honoring the framework and being a prisoner of it.
The second specific difference: quantification instinct. The prompt's Rule 4 says "convert 'handles lots of data' → specific numbers." Weaker models would add numbers but anchor them to nothing — "processes millions of transactions" — which is technically a number but carries zero engineering information. The stronger behavior was tracing the number back to a verifiable source and flagging uncertainty appropriately: "estimated 1,900 TPS average, ~8,000–10,000 TPS at peak — derived from NPCI public monthly volume data, not confirmed by PhonePe." That's not just a more specific answer. It's a different epistemic posture — the model is showing its work and marking the confidence boundary, which is exactly what a senior engineer would do in a design doc they're willing to put their name on.
