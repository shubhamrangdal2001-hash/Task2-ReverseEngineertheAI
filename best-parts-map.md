| Layer                                    | Best LLM for This Layer | What to Extract (copy the key paragraph/section)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ---------------------------------------- | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Layer 1: Data Foundation**             | **Claude**              | "Perplexity operates a hybrid data architecture. For real-time queries, it calls third-party search APIs (Bing Web Search API is the most likely primary source, given licensing) to get a SERP of URLs ranked by relevance. It then fires **parallel async HTTP fetches** against the top 5--10 URLs, runs HTML stripping and boilerplate removal (nav bars, footers, cookie banners) to extract clean article text, and chunks that text into passage segments of roughly 200--400 tokens."                                                                                                                |
|                                          |                         | **Engineering challenge:** "Web pages are **hostile to scraping at speed**. JavaScript-rendered SPAs, paywalls, Cloudflare bot protection, and wildly inconsistent HTML structure mean your content extraction pipeline needs to be fault-tolerant with fallback strategies. At 10M+ queries/day, you're making 50--100M+ outbound HTTP requests daily --- IP rotation, rate limit management, and fetch timeouts become first-class infrastructure concerns."                                                                                                                                               |
|                                          |                         | **Honesty check:** "This layer is **heavily used and underappreciated**. Most people think Perplexity is 'just an LLM wrapper.' The data foundation --- specifically real-time page fetching and cleaning --- is where a huge amount of engineering lives."                                                                                                                                                                                                                                                                                                                                                  |
| **Layer 2: Statistics & Analysis**       | **Claude**              | "Perplexity runs a **continuous A/B experimentation platform** across multiple surfaces: which LLM to use for a given query type, how many sources to retrieve, reranker thresholds, and prompt templates. The core product metrics they're optimizing are likely **answer acceptance rate** (does the user follow up, copy, click citations, or bounce?), **session depth**, and **query reformulation rate** (a proxy for dissatisfaction)."                                                                                                                                                               |
|                                          |                         | **Engineering challenge:** "LLM outputs are **non-deterministic and hard to measure automatically**. Traditional A/B metrics like CTR don't fully capture 'was this answer good?' You need a blend of implicit signals (copy to clipboard, citation click, session continuation) and explicit signals (thumbs up/down) to build a reliable reward signal --- and these signals are **sparse and delayed**, making statistical power a real problem at lower query volumes."                                                                                                                                  |
|                                          |                         | **Honesty check:** "This layer is **moderately used** --- it's real and important for product iteration, but it's not a core differentiator. Perplexity's competitive advantage isn't statistical sophistication; it's retrieval quality and latency."                                                                                                                                                                                                                                                                                                                                                       |
| **Layer 3: ML Models**                   | **Claude**              | "There are **three distinct ML subsystems** here. First, a **query classifier** that routes queries to different pipeline configurations --- likely a fine-tuned **BERT-class encoder**. Second, a **neural reranker** that takes the top-N retrieved passages and scores them for relevance --- almost certainly a **cross-encoder architecture** rather than a bi-encoder, because cross-encoders are significantly more accurate at the cost of being slower. Third, a **citation attribution model** (or heuristic-ML hybrid) that maps spans of the generated answer back to specific source passages." |
|                                          |                         | **Engineering challenge:** "The reranker sits on the **critical latency path** --- it runs after retrieval but before generation, and it needs to score potentially 50--100 passage chunks against the query in under 300ms. Cross-encoders are accurate but slow; you're forced into aggressive optimizations: ONNX runtime export, INT8 quantization, batching passage scoring across concurrent requests, and careful GPU memory management."                                                                                                                                                             |
|                                          |                         | **Honesty check:** "This layer is **genuinely critical and often invisible** in product teardowns. People focus on the LLM layer, but a bad reranker means the LLM gets garbage context and produces garbage answers."                                                                                                                                                                                                                                                                                                                                                                                       |
| **Layer 4: LLM / Generative AI**         | **Claude**              | "Perplexity runs a **multi-model architecture** --- they publicly use Claude 3.5/3.7, GPT-4o, Sonar (their own fine-tuned Llama-based model), and Gemini. Their default **Sonar models** are Llama 3.x base models fine-tuned specifically for **grounded generation**: trained to only make claims attributable to retrieved context, format citations inline, and structure answers appropriately. For 'Pro Search,' they add an **agentic loop** --- the model can issue sub-queries and iteratively refine retrieval."                                                                                   |
|                                          |                         | **Engineering challenge:** "**Faithfulness at speed.** You need the model to: (a) only claim what's in retrieved context, (b) correctly attribute claims, (c) do this while streaming tokens in <2s. These goals are in tension. Fine-tuning Sonar on synthetic preference data using **DPO (Direct Preference Optimization)** is the most plausible approach to baking in citation behavior without RLHF's expensive reward model infrastructure."                                                                                                                                                          |
|                                          |                         | **Key technologies:** "Sonar (Llama 3.x fine-tuned), Claude 3.5/3.7 Sonnet, GPT-4o, **vLLM or TGI** for self-hosted inference, **custom RAG orchestration** (likely in-house, not LangChain --- LangChain is too slow for sub-2s targets), **Streaming via SSE**."                                                                                                                                                                                                                                                                                                                                           |
| **Layer 5: Deployment & Infrastructure** | **Claude**              | "Perplexity's serving infrastructure orchestrates a **fan-out/fan-in request pattern** under strict latency budgets: search API call (~200ms), 5--10 parallel page fetches (~500ms), passage extraction (~50ms), reranker scoring (~200ms), LLM generation (~1--3s). These happen in a partially parallelized DAG. They run **AWS or mix of AWS + on-prem GPU clusters** --- Sonar runs on their own H100/A100 clusters (likely CoreWeave or Lambda Labs), while third-party calls use API with fallback routing."                                                                                           |
|                                          |                         | **Engineering challenge:** "**The P99 latency problem.** Median latency might be 2s, but P99 could be 8s+ if any leg times out poorly. Engineering robust **timeout cascades** --- where each stage has a hard deadline and gracefully degrades (fewer sources, skip reranker, use smaller model) --- is genuinely hard distributed systems work."                                                                                                                                                                                                                                                           |
|                                          |                         | **Skill required:** "Senior SRE with experience in high-concurrency async architectures; GPU cluster management and inference serving (vLLM, Triton); distributed systems debugging (tracing with Jaeger/Zipkin); experience building latency-sensitive pipelines with graceful degradation."                                                                                                                                                                                                                                                                                                                |
| **Layer 6: System Design & Scale**       | **Claude**              | "At reported scale (~100M queries/month), Perplexity handles ~40--50 QPS average with spikes. The architecture needs **three tiers of caching**: (1) **SERP caching** --- popular queries skip Bing API; (2) **URL content caching** --- a BBC article fetched for one user doesn't get re-fetched for the next 5; (3) **Answer caching** --- exact or near-duplicate queries return cached answers (fuzzy-matched via SimHash or MinHash)."                                                                                                                                                                 |
|                                          |                         | **Engineering challenge:** "**Cache invalidation at the content layer.** Web content changes --- a news article updated 10 minutes ago should invalidate cached passages. The heuristic solution is **TTL-based expiration with topic-sensitivity tiers**: 'Eiffel Tower history' = 24h TTL; news/finance/sports = 5-minute TTL. Getting these wrong means serving stale answers on breaking news."                                                                                                                                                                                                          |
|                                          |                         | **Honesty check:** "This layer is **real and non-trivial**, but Perplexity is not operating at Google-scale. The unique challenge is the **cost structure**: every query consumes search API credits + GPU compute + HTTP fetches. Unit economics force aggressive caching in a way pure LLM chatbots don't face."                                                                                                                                                                                                                                                                                           |
| **Overall Analysis / Hardest Problem**   | **Claude**              | **Most Critical Layer:** "Layer 4 (LLM/RAG) and Layer 3 (Reranking) are co-equal and inseparable... The **reranker â†’ LLM handoff** is the single most critical interface in the system."                                                                                                                                                                                                                                                                                                                                                                                                                     |
|                                          |                         | **Complexity Rating:** "Advanced (approaching Bleeding Edge in specific subsystems)... What makes this *Advanced* is: running them together under a **hard 2--3 second latency budget**, the **real-time web fetching** component, **Sonar fine-tuning for citation behavior**, and the **cost optimization layer**."                                                                                                                                                                                                                                                                                        |
|                                          |                         | **If rebuilding from scratch:** "**The content extraction and passage quality pipeline.** Not the LLM. Not the reranker. The boring, unsexy HTML-fetching-and-cleaning infrastructure... A team that gets extraction right and uses GPT-3.5 will **outperform** a team with GPT-4o on top of bad extraction."                                                                                                                                                                                                                                                                                                |
| **Writing Style / Structure**            | **ChatGPT**             | **Format to adopt:** Consistent markdown headers, bullet-pointed technology lists, clear 5-subsection structure per layer, "Confidence: X%" indicator at end. ChatGPT's structure was more scannable and interview-ready.                                                                                                                                                                                                                                                                                                                                                                                    |
| **Uncertainty Calibration**              | **Claude**              | **Technique to adopt:** Consistent use of "likely", "probably", "almost certainly", "I suspect" for internal architecture; explicit "Honesty check" nuance ("moderately used" vs "heavily used"); avoidance of stating uncertain things as fact.                                                                                                                                                                                                                                                                                                                                                             |
