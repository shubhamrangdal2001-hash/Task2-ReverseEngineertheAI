# LLM Comparison — Product Teardown: Perplexity.ai's search answers

Models Used
| # | LLM Name          | Mode (Fast/Standard/Thinking) | Response Time (approx) |
| - | ----------------- | ----------------------------- | ---------------------- |
| 1 | ChatGPT-4o        | Thinking                      | ~6s                    |
| 2 | Claude 3.5 Sonnet | Extended Thinking             | ~8s                    |
| 3 | Gemini 1.5 Pro    | Standard/Thinking             | N/A - Failed           |

Layer-by-Layer Comparison
Layer 1: Data Foundation
| Criteria                                 | LLM 1 (ChatGPT)                                           | LLM 2 (Claude)                                                                                                                                                             | LLM 3 (Gemini) | Best?                                                                                                               |
| ---------------------------------------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- | ------------------------------------------------------------------------------------------------------------------- |
| Specificity (1-5)                        | 4                                                         | 5                                                                                                                                                                          | N/A            | Claude                                                                                                              |
| Named real tech?                         | Y                                                         | Y                                                                                                                                                                          | N/A            | Claude                                                                                                              |
| Identified a real engineering challenge? | Y                                                         | Y                                                                                                                                                                          | N/A            | Claude                                                                                                              |
| Notes                                    | Cited Vespa.ai, mentioned Kafka, S3. Good on public info. | Exceptional: identified "web pages are hostile to scraping" as core challenge, named Playwright/Puppeteer, Trafilatura, specific API (Bing). Best uncertainty calibration. | **No output**  | **Claude wins** - deepest on extraction pipeline, identified third-party search API dependency (Bing) vs. own crawl |

Layer 2: Statistics & Analysis
| Criteria                                 | LLM 1 (ChatGPT)                                    | LLM 2 (Claude)                                                                                                                                                                                            | LLM 3 (Gemini) | Best?                                                                                |
| ---------------------------------------- | -------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- | ------------------------------------------------------------------------------------ |
| Specificity (1-5)                        | 3                                                  | 4                                                                                                                                                                                                         | N/A            | Claude                                                                               |
| Named real tech?                         | Y                                                  | Y                                                                                                                                                                                                         | N/A            | Claude                                                                               |
| Identified a real engineering challenge? | Y                                                  | Y                                                                                                                                                                                                         | N/A            | Claude                                                                               |
| Notes                                    | Generic A/B testing description, standard metrics. | Identified "sparse and delayed signals" problem, named Statsig/LaunchDarkly, CUPED variance reduction, Thompson Sampling. **Honesty check nuance**: correctly called this "moderately used" not critical. | **No output**  | **Claude wins** - only one to identify statistical power problem with LLM evaluation |

Layer 3: Machine Learning Models
| Criteria                                 | LLM 1 (ChatGPT)                                                        | LLM 2 (Claude)                                                                                                                                                                                                                                     | LLM 3 (Gemini) | Best?                                                                                                                        |
| ---------------------------------------- | ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| Specificity (1-5)                        | 4                                                                      | 5                                                                                                                                                                                                                                                  | N/A            | Claude                                                                                                                       |
| Named real tech?                         | Y                                                                      | Y                                                                                                                                                                                                                                                  | N/A            | Claude                                                                                                                       |
| Named model family?                      | Y                                                                      | Y                                                                                                                                                                                                                                                  | N/A            | Claude                                                                                                                       |
| Identified a real engineering challenge? | Y                                                                      | Y                                                                                                                                                                                                                                                  | N/A            | Claude                                                                                                                       |
| Notes                                    | Good: Sentence-BERT, cross-encoders, ONNX. Mentioned latency tradeoff. | Exceptional: identified **three distinct subsystems** (query classifier, neural reranker, citation attribution), named BGE-Reranker-Large, DeBERTa, MiniLM. Specific on "cross-encoder vs bi-encoder tradeoffs", INT8 quantization, ONNX/TensorRT. | **No output**  | **Claude wins** - only one to identify citation attribution as separate ML problem; best on inference optimization specifics |

Layer 4: LLM / Generative AI
| Criteria                  | LLM 1 (ChatGPT)                                                                              | LLM 2 (Claude)                                                                                                                                                                                                                                          | LLM 3 (Gemini) | Best?                                                                                                                                 |
| ------------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| Specificity (1-5)         | 4                                                                                            | 5                                                                                                                                                                                                                                                       | N/A            | Claude                                                                                                                                |
| Honest if not applicable? | N/A (applicable)                                                                             | N/A (applicable)                                                                                                                                                                                                                                        | N/A            | Tie                                                                                                                                   |
| Notes                     | Good coverage of multi-model architecture, Sonar models, RAG. Mentioned vLLM, SSE streaming. | Superior depth: identified **Sonar fine-tuning methodology** (DPO vs RLHF), specific model variants (Claude 3.5/3.7, GPT-4o), "Constitutional AI-style RLHF", custom RAG orchestration (not LangChain). Best: "faithfulness at speed" tension analysis. | **No output**  | **Claude wins** - only one to explain *why* they built custom orchestration (LangChain too slow); identified DPO as training approach |

Layer 5: Deployment & Infrastructure
| Criteria          | LLM 1 (ChatGPT)                                              | LLM 2 (Claude)                                                                                                                                                                                                                                       | LLM 3 (Gemini) | Best?                                                                                                                |
| ----------------- | ------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- | -------------------------------------------------------------------------------------------------------------------- |
| Specificity (1-5) | 3                                                            | 4                                                                                                                                                                                                                                                    | N/A            | Claude                                                                                                               |
| Named real tech?  | Y                                                            | Y                                                                                                                                                                                                                                                    | N/A            | Claude                                                                                                               |
| Notes             | Standard Kubernetes, GPU fleets, Redis. Generic description. | Better: named **CoreWeave or Lambda Labs** (GPU cloud providers), identified "fan-out/fan-in request pattern", specific on "async Python (FastAPI + asyncio) or Go" for orchestration. **Key insight**: "P99 latency problem" with timeout cascades. | **No output**  | **Claude wins** - identified specific GPU cloud providers and language choice tradeoffs; best on latency engineering |


Layer 6: System Design & Scale
| Criteria          | LLM 1 (ChatGPT)                                                           | LLM 2 (Claude)                                                                                                                                                                                                                                                                | LLM 3 (Gemini) | Best?                                                                                                                  |
| ----------------- | ------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- | ---------------------------------------------------------------------------------------------------------------------- |
| Specificity (1-5) | 3                                                                         | 4                                                                                                                                                                                                                                                                             | N/A            | Claude                                                                                                                 |
| Named real tech?  | Y                                                                         | Y                                                                                                                                                                                                                                                                             | N/A            | Claude                                                                                                                 |
| Notes             | Standard caching, multi-region, CDN. Mentioned ~100M queries/month scale. | Better: identified **three-tier caching strategy** (SERP, URL content, Answer), named SimHash/MinHash LSH for fuzzy matching, specific on TTL heuristics (24h vs 5min by topic). **Honesty check nuance**: "not Google-scale" but "cost structure forces aggressive caching." | **No output**  | **Claude wins** - only one to detail cache invalidation heuristics and near-duplicate detection; realistic about scale |

Overall Verdict
| Dimension                          | Winner (LLM #)  | Why? (1 sentence)                                                                                                                                                                       |
| ---------------------------------- | --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Most technically specific overall  | **2 (Claude)**  | Consistently named specific tools (BGE-Reranker, Trafilatura, CoreWeave, SimHash) and architectural patterns (fan-out/fan-in, DPO training) that others glossed over.                   |
| Best at naming real technologies   | **2 (Claude)**  | Named 15+ specific tools/frameworks with plausible uncertainty calibration; ChatGPT was more generic (Kubernetes, Redis); Gemini failed.                                                |
| Least hallucination / made-up info | **2 (Claude)**  | Best uncertainty calibration ("likely", "probably", "I suspect") and explicit honesty checks; ChatGPT occasionally stated uncertain things as fact (Vespa.ai usage).                    |
| Best at "hardest problem" insight  | **2 (Claude)**  | Identified counterintuitive insight: content extraction pipeline is most critical, not LLM; identified "faithfulness at speed" tension and "P99 latency problem" with timeout cascades. |
| Best structured output             | **1 (ChatGPT)** | Most consistent formatting, clear headers, bullet points; Claude was more narrative and exceeded length constraints; Gemini failed.                                                     |
| Fastest useful response            | **1 (ChatGPT)** | ~6s vs ~8s for Claude; Gemini never completed.                                                                                                                                          |


Key Observation
One thing I noticed about how different LLMs handle the same prompt: 
Claude's extended thinking mode produced the deepest technical analysis but required more parsing due to narrative style, 
while ChatGPT's thinking mode prioritized structure and speed over depth.
Claude was the only model to challenge the prompt's implicit assumptions—for example, identifying that Perplexity likely uses Bing API rather than own crawl, and that content extraction (not the LLM) is the critical path.
Gemini failed entirely despite an aggressive persona, suggesting that persona adoption doesn't correlate with task completion capability.
